# ğŸ§  MentorMind

> [ğŸ‡»ğŸ‡³ Tiáº¿ng Viá»‡t](README.vi.md)

A personal Telegram bot for active learning â€” auto-syncs articles from Raindrop.io, analyzes them with AI multi-persona pipeline, and generates weekly insight reports.

## âœ¨ Features

| Feature | Description |
|---|---|
| **Raindrop Sync** | Auto-sync articles from Raindrop.io collection |
| **Smart Extraction** | Extract content from articles, YouTube, GitHub. Uses [Camofox](#-camofox) for Facebook/LinkedIn |
| **AI Multi-Persona Analysis** | Analysis through 4 personas: Scout â†’ Builder â†’ Debater â†’ Chief |
| **Batch Overview** | Summarize 2-10 articles at once, detect common themes |
| **Reflection System** | Guided reflection after each article â€” record insights + action items + confidence score |
| **Weekly Synthesis** | Weekly report: themes, knowledge gaps, recommendations for next week |
| **Streak Tracking** | Daily learning streak + session timer |
| **Scheduled Jobs** | Daily auto sync+analyze + Sunday weekly report |

## ğŸ—ï¸ Architecture

<p align="center">
  <img src="docs/images/architecture.svg" alt="Architecture Overview" width="800"/>
</p>

##  Screenshots

<p align="center">
  <img src="docs/images/1.png" width="800" />
  <img src="docs/images/2.png" width="800" />
  <img src="docs/images/3.png" width="800" />
  <img src="docs/images/4.png" width="800" />
</p>

## ğŸ“ Project Structure

```
â”œâ”€â”€ bot/
â”‚   â””â”€â”€ telegram_handler.py   # Telegram command handlers
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ raindrop.py            # Raindrop.io sync
â”‚   â”œâ”€â”€ extractor.py           # Content extraction (article/YouTube/GitHub)
â”‚   â”œâ”€â”€ analyzer.py            # Multi-persona LLM analysis
â”‚   â”œâ”€â”€ digest.py              # Batch overview service
â”‚   â”œâ”€â”€ synthesizer.py         # Weekly synthesis service
â”‚   â”œâ”€â”€ llm_client.py          # LLM client with fallback chain
â”‚   â””â”€â”€ scheduler.py           # APScheduler daily + weekly jobs
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ models.py              # SQLite schema
â”‚   â””â”€â”€ repository.py          # Database operations
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ personas/              # 4 persona prompts (Scout, Builder, Debater, Chief)
â”‚   â”œâ”€â”€ digest.md              # Batch overview prompt
â”‚   â””â”€â”€ weekly.md              # Weekly synthesis prompt
â”œâ”€â”€ config.py                  # Configuration from .env
â”œâ”€â”€ main.py                    # Entry point
â””â”€â”€ scripts/                   # Utility scripts
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Telegram Bot Token ([BotFather](https://t.me/BotFather))
- Raindrop.io API Token ([raindrop.io/settings/integrations](https://raindrop.io/settings/integrations))
- LLM API endpoint (OpenAI-compatible, e.g. Antigravity Tools Proxy)
- [Camofox Browser](https://github.com/jo-inc/camofox-browser) â€” for Facebook/LinkedIn extraction

### Installation

```bash
# Clone
git clone <repo-url>
cd 1-Personal-AI-Learning-Assistant

# Install dependencies
pip install -r requirements.txt

# Configure
cp .env.example .env
# Edit .env with your tokens
```

### Configuration (.env)

```env
# Required
TELEGRAM_BOT_TOKEN=your-telegram-bot-token
TELEGRAM_CHAT_ID=your-chat-id
RAINDROP_API_TOKEN=your-raindrop-token

# LLM Gateway
ANTIGRAVITY_API_KEY=sk-antigravity
ANTIGRAVITY_BASE_URL=http://127.0.0.1:8045/v1

# Scheduler (daily auto job)
SCHEDULE_HOUR=8
SCHEDULE_MINUTE=0
SCHEDULE_ENABLED=true

# Timezone
TZ=Asia/Ho_Chi_Minh

# Camofox Browser (extractor for Facebook/LinkedIn posts)
CAMOFOX_URL=http://localhost:9377
CAMOFOX_USER_ID=learning-bot
CAMOFOX_API_KEY=<your_api_key>
```

### Run

```bash
python main.py
```

## ğŸ“± Commands

### ğŸ“– Learning
| Command | Description |
|---|---|
| `/analyze` | Analyze next article (latest, highest priority) |
| `/analyze <id>` | Analyze specific article by DB ID |
| `/next` | Preview next article (without analyzing) |
| `/skip` | Skip next article |
| `/overview` | Overview of 5 latest queued articles |
| `/overview <n>` | Overview of n articles (2-10) |
| `/reflect` | Reflect on the last read article |
| `/reflect <id>` | Reflect on specific article |
| `/weekly` | Generate weekly learning synthesis |

### â±ï¸ Tracking
| Command | Description |
|---|---|
| `/session start` | Start a learning session |
| `/session stop` | End session |
| `/session` | View current session |
| `/status` | Stats + streak |

### âš™ï¸ Management
| Command | Description |
|---|---|
| `/sync` | Sync new articles from Raindrop |
| `/schedule` | View/change auto schedule |
| `/help` | List all commands |

## ğŸ”„ Learning Flow

<p align="center">
  <img src="docs/images/learning workflow.png" alt="Learning Flow Comparison" width="800"/>
</p>


## ğŸ”¬ 2-Layer Intelligent Extraction

The extractor uses a **2-layer optimization pipeline** to ensure content quality:

**Layer 1 â€” Raw Extraction** (select best source):
```
URL â†’ Detect content type
  â”‚
  â”œâ”€ Walled garden? â†’ Camofox (full render) â†’ OG meta (fallback)
  â”œâ”€ YouTube?       â†’ Transcript API (vi â†’ en â†’ auto)
  â””â”€ Article?       â†’ Trafilatura (primary) â†’ Jina Reader (fallback)
```

**Layer 2 â€” Content Optimization** (clean + enrich):
```
Raw content â†’ Short content? (<100 words)
  â”œâ”€ yes â†’ Find URLs in content/excerpt â†’ Follow link â†’ Re-extract
  â””â”€ Camofox snapshot? â†’ clean_camofox_snapshot() (strip UI noise, keep real text)
      â†’ Raw 10K chars â†’ Cleaned ~3K chars (60-70% reduction)
```

> When original content is too short (social posts sharing links), Layer 2 automatically follows URLs in the content to fetch the full article.

## ğŸ¤– AI Multi-Persona Analysis

Each article goes through a 2-stage analysis pipeline:

**Stage 1 â€” Research:**
- ğŸ”¬ **Scout** (Researcher): Explores and analyzes technical content, extracts key insights

**Stage 2 â€” Synthesis:**
- ğŸ—ï¸ **Builder** (Architect): System design, evaluates practical applicability
- ğŸ¤” **Debater** (Skeptic): Challenges claims, asks hard questions, finds gaps
- ğŸ“ **Chief** (Synthesizer): Synthesizes insights, delivers final conclusions

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|---|---|
| Bot Framework | python-telegram-bot 21+ |
| LLM Client | OpenAI SDK (via proxy) |
| Database | SQLite |
| Scheduler | APScheduler |
| Content Extraction | Trafilatura, BeautifulSoup4, youtube-transcript-api |
| HTTP | httpx |

## ğŸ“… Scheduled Jobs

| Job | Schedule | Action |
|---|---|---|
| Daily Sync & Analyze | Configurable (default 08:00) | Sync Raindrop â†’ analyze 1 article â†’ send result |
| Weekly Synthesis | Sunday 23:00 | Weekly report â†’ send to Telegram |

## ğŸ¦Š Camofox

The bot uses [Camofox Browser](https://github.com/jo-inc/camofox-browser) â€” a headless browser built on **Camoufox** (Firefox fork with C++ anti-detection) â€” to extract content from bot-protected platforms.

**Supported**: Facebook posts/reels, LinkedIn articles, Instagram

**Features**:
- Full page render with anti-detection (bypass bot checks)
- Accessibility snapshot â†’ `clean_camofox_snapshot()` strips UI noise, keeps real text
- Screenshot for multimodal Gemini analysis
- Link extraction (detect links in comments)

### Facebook Cookie Import

By default, Camofox can access public content. To read **full content** (comments, full post text, private groups), import Facebook cookies:

```bash
# 1. Install "cookies.txt" browser extension (Chrome/Firefox)
# 2. Export cookies for facebook.com (Netscape format)
# 3. Place in Camofox directory
mkdir -p ~/.camofox/cookies
cp ~/Downloads/facebook_cookies.txt ~/.camofox/cookies/facebook.txt
```

With cookies imported, Camofox renders the page as a logged-in user â€” giving access to full post content + comments.

See detailed setup: [`docs/camofox_setup.md`](docs/camofox_setup.md)

## ğŸ“ OpenSpec â€” Spec-Driven Development

This project is developed using the **spec-driven** methodology with [OpenSpec](https://github.com/9aia/openspec) â€” a framework for managing changes through artifacts (proposal â†’ design â†’ specs â†’ tasks).

### Why OpenSpec?

- **Traceability**: Every feature has a full proposal, design rationale, behavioral specs, and task breakdown
- **Incremental delivery**: Changes are broken into scoped, manageable units
- **Living documentation**: Specs reflect the current state of the system, not write-once-forget docs

### Workflow customization

OpenSpec's default workflows are customized in `.agent/workflows/`. One notable change:

**`opsx-archive.md`** â€” added guardrail:
```
ALWAYS update openspec/ROADMAP.md after archiving
```

Reason: ROADMAP.md is the single source of truth for project progress. Without this rule, archiving a change would leave ROADMAP out of sync with reality. This guardrail ensures that after every archive, the change is automatically moved from "Remaining" to "Completed" with the archive date.

### Project structure

```
openspec/
â”œâ”€â”€ ROADMAP.md              # Overall progress â€” single source of truth
â”œâ”€â”€ specs/                  # Main specs (behavioral requirements)
â”‚   â”œâ”€â”€ article-flow/
â”‚   â”œâ”€â”€ telegram-bot/
â”‚   â”œâ”€â”€ weekly-synthesis/
â”‚   â””â”€â”€ ...
â””â”€â”€ changes/
    â””â”€â”€ archive/            # Completed changes (10 changes archived)
        â”œâ”€â”€ 2026-02-15-select-llm-model/
        â”œâ”€â”€ 2026-02-15-setup-database/
        â””â”€â”€ ...
```

### Lessons learned

OpenSpec excels at managing individual changes, but has 2 weaknesses for multi-phase projects:

| Weakness | Problem | Workaround |
|---|---|---|
| **No overall progress tracking** | Each change is a silo, no "project dashboard" | Created `ROADMAP.md` manually + guardrail to auto-update on archive |
| **No auto-merge of specs** | Delta specs from multiple changes don't auto-merge into main spec | Run `/opsx-sync` before archive â€” but easy to forget (happened once) |

If you use OpenSpec for a large project, we recommend:
1. Create `ROADMAP.md` from the start
2. Add "update ROADMAP" guardrail to your archive workflow
3. Always sync delta specs before archiving
