## Why

Há»‡ thá»‘ng cáº§n chá»n LLM model cá»¥ thá»ƒ Ä‘á»ƒ implement. Hiá»‡n táº¡i config ghi "TBD". User Ä‘Ã£ cÃ³ license **Google AI Ultra** ($249.99/thÃ¡ng) vá»›i quota Ä‘áº§y Ä‘á»§ â€” táº­n dá»¥ng tÃ i nguyÃªn cÃ³ sáºµn, **$0 chi phÃ­ thÃªm**.

## What Changes

- Chá»‘t LLM model theo **2-stage pipeline** (phÃ¢n tÃ­ch â†’ planning)
- XÃ¡c Ä‘á»‹nh API endpoints vÃ  integration method
- Cáº­p nháº­t `config.py` vÃ  `.env` vá»›i model configuration
- Design `services/analyzer.py` vá»›i 2-stage routing logic + fallback chain

## Quota hiá»‡n cÃ³ (Google AI Ultra â€” $0 chi phÃ­ thÃªm)

| Model | Quota | Reset |
|-------|-------|-------|
| **Gemini 3 Pro** (High/Low) | 100% | ~5h |
| **Gemini 3 Flash** | 100% | ~5h |
| **Claude Opus 4.5/4.6** (Thinking) | 100% | ~5h |
| Claude Sonnet 4.5 | 100% | ~5h |
| GPT-OSS 120B | 100% | ~5h |

> Opus vÃ  Sonnet cÃ¹ng quota â†’ dÃ¹ng Opus (model máº¡nh nháº¥t).

## 2-Stage Pipeline Design

### Táº¡i sao 2-stage thay vÃ¬ 1-call?

Má»—i model cÃ³ Ä‘iá»ƒm máº¡nh khÃ¡c nhau:
- **Gemini 3 Pro**: MMMU 81%, tiáº¿ng Viá»‡t hÃ ng Ä‘áº§u, multimodal â†’ **hiá»ƒu bÃ i tá»‘t nháº¥t**
- **Claude Opus 4.6**: ARC-AGI 37.6%, SWE-Bench 80.9% â†’ **reasoning + planning tá»‘t nháº¥t**

TÃ¡ch 2 stage Ä‘á»ƒ má»—i model chÆ¡i Ä‘Ãºng sá»Ÿ trÆ°á»ng, trÃ¡nh "vá»«a Ä‘Ã¡nh trá»‘ng vá»«a thá»•i cÃ²i".

### Stage 1: PHÃ‚N TÃCH (Gemini 3 Pro â€” 1 call)

Gá»­i bÃ i viáº¿t â†’ Gemini tráº£ 3 persona cÃ¹ng lÃºc:

```
ğŸ“¥ Input: BÃ i viáº¿t "RAG Pipeline Best Practices 2026"

ğŸ“¤ Output (Gemini 3 Pro):

ğŸ”¬ RESEARCHER:
- TÃ³m táº¯t: BÃ i viáº¿t trÃ¬nh bÃ y best practices cho RAG pipeline...
- Key insights:
  â€¢ Chunking strategy áº£nh hÆ°á»Ÿng 80% cháº¥t lÆ°á»£ng
  â€¢ Hybrid search (keyword + semantic) outperform pure vector
- Concept: Giáº£i thÃ­ch "late chunking"...

ğŸ—ï¸ ARCHITECT:
- System design impact: CMS cáº§n tÃ¡ch indexing thÃ nh async pipeline...
- á»¨ng dá»¥ng: eCommerce search cÃ³ thá»ƒ dÃ¹ng hybrid search...
- Risk: Vector DB scaling khi >1M documents...

ğŸ” SKEPTIC:
- Hype: "80%" lÃ  sá»‘ liá»‡u tá»« 1 benchmark cá»¥ thá»ƒ, khÃ´ng phá»• quÃ¡t
- Giá»›i háº¡n: RAG váº«n struggle vá»›i multi-hop reasoning
- Alternative: Fine-tuning on domain data cÃ³ thá»ƒ tá»‘t hÆ¡n RAG
```

â†’ 3 gÃ³c nhÃ¬n, cÃ¹ng 1 model â†’ style nháº¥t quÃ¡n, khÃ´ng mÃ¢u thuáº«n.

### Stage 2: Tá»”NG Há»¢P & Káº¾ HOáº CH (Claude Opus 4.6 â€” 1 call)

Gá»­i output Stage 1 â†’ Opus tá»•ng há»£p + lÃªn plan:

```
ğŸ“¥ Input: Output Stage 1 (Researcher + Architect + Skeptic)

ğŸ“¤ Output (Claude Opus 4.6):

ğŸ“Š SYNTHESIZER:
1. Chunking strategy lÃ  bottleneck #1 â€” Ä‘áº§u tÆ° thá»i gian á»Ÿ Ä‘Ã¢y
2. Hybrid search lÃ  safe bet cho production
3. RAG khÃ´ng pháº£i silver bullet â€” evaluate vs fine-tuning
4. Async indexing pipeline nÃªn lÃ  pattern máº·c Ä‘á»‹nh
5. Cáº§n benchmark trÃªn data tháº­t, khÃ´ng tin sá»‘ liá»‡u paper

ğŸ¯ ACTION PLAN:
- Tuáº§n nÃ y: Build mini POC so sÃ¡nh 3 chunking strategies
- Äá»c thÃªm: "Late Chunking" paper
- Ãp dá»¥ng: Thá»­ hybrid search cho project hiá»‡n táº¡i
```

â†’ Opus dÃ¹ng reasoning máº¡nh Ä‘á»ƒ ra plan cá»¥ thá»ƒ, actionable hÆ¡n Gemini.

### So sÃ¡nh 1-call vs 2-stage

| | 1-call (cÅ©) | 2-stage (má»›i) |
|---|------------|--------------|
| Gemini lÃ m gÃ¬ | Cáº£ 4 persona | 3 persona (phÃ¢n tÃ­ch) |
| Opus lÃ m gÃ¬ | KhÃ´ng dÃ¹ng | Synthesizer + Action Plan |
| Tá»•ng calls | 1 | 2 |
| Latency | ~10s | ~20s |
| Action quality | OK | **Máº¡nh hÆ¡n nhiá»u** |
| Coherence | Cao | Cao (má»—i stage 1 model) |

### Model Config

```python
MODEL_CONFIG = {
    # Stage 1: PhÃ¢n tÃ­ch (Gemini 3 Pro)
    "stage_1_analysis": "gemini-3-pro",    # Researcher + Architect + Skeptic

    # Stage 2: Tá»•ng há»£p & Plan (Claude Opus 4.6)
    "stage_2_planning": "claude-opus-4.6",  # Synthesizer + Action Planning

    # CÃ¡c task khÃ¡c
    "batch_digest":     "gemini-3-pro",     # TÃ³m táº¯t backlog
    "weekly_synthesis": "gemini-3-pro",     # Cross-article analysis

    # Fallback chain (khi primary lá»—i/rate limit)
    "fallback_chain": [
        "gemini-3-flash",    # 95% quality cá»§a Pro, 3x nhanh
        "claude-opus-4.5",   # Backup reasoning
        "gpt-oss-120b",      # Last resort
    ]
}
```

## Capabilities

### New Capabilities
- `llm-config`: Cáº¥u hÃ¬nh 2-stage model pipeline, API keys, fallback chain, parameters

### Modified Capabilities
_KhÃ´ng cÃ³ â€” chÆ°a cÃ³ spec nÃ o existing_

## Impact

- `config.py`: 2-stage model config + fallback chain
- `.env` / `.env.example`: `GEMINI_API_KEY`, `ANTHROPIC_API_KEY`
- `services/analyzer.py`: 2-stage pipeline logic + fallback
- `requirements.txt`: `google-generativeai`, `anthropic`

## KhÃ´ng lÃ m

- âŒ KhÃ´ng dÃ¹ng model khÃ¡c nhau cho tá»«ng persona trong cÃ¹ng 1 stage
- âŒ KhÃ´ng implement model switching UI/command
- âŒ KhÃ´ng benchmark models
- âŒ KhÃ´ng setup OpenRouter/external proxy
